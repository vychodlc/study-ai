# 模块4: 开发框架

## 课程概述

本模块全面介绍AI应用开发中常用的框架和工具。从LangChain这一最流行的LLM应用框架入手，深入理解AI框架的设计原理，然后扩展到HuggingFace生态、神经网络基础以及PyTorch和TensorFlow的实战应用。

---

## 第一章：LangChain多任务应用开发

### 1.1 LangChain核心概念

#### 1.1.1 LangChain是什么

**LangChain定义：**

LangChain是一个用于构建大语言模型应用的开源框架，它提供了模块化的组件，让开发者能够轻松地将LLM与各种数据源、工具和服务连接起来。

```
┌─────────────────────────────────────────────────────────────┐
│                   LangChain架构                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐    │
│  │                  LangChain Core                      │    │
│  │  基础抽象层：定义核心接口和数据结构                    │    │
│  └─────────────────────────────────────────────────────┘    │
│                           ↑                                 │
│  ┌─────────────────────────────────────────────────────┐    │
│  │                六大核心组件                          │    │
│  ├─────────┬─────────┬─────────┬─────────┬─────────────┤    │
│  │ Models  │ Prompts │ Memory  │ Indexes │ Chains      │    │
│  │ 模型层  │ 提示词  │ 记忆    │ 索引    │ 链条        │    │
│  ├─────────┴─────────┴─────────┴─────────┼─────────────┤    │
│  │                                       │   Agents    │    │
│  │                                       │   智能体    │    │
│  └───────────────────────────────────────┴─────────────┘    │
│                           ↑                                 │
│  ┌─────────────────────────────────────────────────────┐    │
│  │                  LangChain Community                 │    │
│  │  社区贡献：各种第三方集成、工具、向量库等              │    │
│  └─────────────────────────────────────────────────────┘    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 1.1.2 六大核心组件详解

**1. Models（模型层）**

```python
from langchain_openai import ChatOpenAI
from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage, SystemMessage

# OpenAI模型
llm = ChatOpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=1000
)

# 本地Ollama模型
local_llm = Ollama(model="llama2")

# 调用模型
messages = [
    SystemMessage(content="你是一个Python专家"),
    HumanMessage(content="解释什么是装饰器")
]

response = llm.invoke(messages)
print(response.content)
```

**2. Prompts（提示词模板）**

```python
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate

# 基础模板
template = PromptTemplate(
    input_variables=["product"],
    template="给{product}起5个创意品牌名"
)

prompt = template.format(product="智能手表")

# Chat模板
chat_template = ChatPromptTemplate.from_messages([
    ("system", "你是一位{role}专家"),
    ("human", "{question}")
])

messages = chat_template.format_messages(
    role="Python",
    question="如何实现单例模式？"
)

# 少样本模板
from langchain_core.prompts import FewShotPromptTemplate

examples = [
    {"input": "开心", "output": "sad"},
    {"input": "大", "output": "small"},
]

example_template = PromptTemplate(
    input_variables=["input", "output"],
    template="输入: {input}\n输出: {output}"
)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_template,
    prefix="将中文翻译成英文反义词：",
    suffix="输入: {input}\n输出:",
    input_variables=["input"]
)

print(few_shot_prompt.format(input="快乐"))
```

**3. Memory（记忆管理）**

```python
from langchain.memory import (
    ConversationBufferMemory,
    ConversationBufferWindowMemory,
    ConversationSummaryMemory,
    ConversationSummaryBufferMemory
)

# 完整对话记忆
buffer_memory = ConversationBufferMemory()
buffer_memory.save_context({"input": "你好"}, {"output": "你好！有什么可以帮你？"})
buffer_memory.save_context({"input": "今天天气怎么样"}, {"output": "今天晴天"})
print(buffer_memory.load_memory_variables({}))

# 滑动窗口记忆（只保留最近k轮）
window_memory = ConversationBufferWindowMemory(k=5)

# 摘要记忆（用LLM压缩历史）
summary_memory = ConversationSummaryMemory(llm=llm)

# 摘要+缓冲混合（最近对话完整保留，历史用摘要）
hybrid_memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=500
)
```

**4. Indexes（索引/向量存储）**

```python
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma

# 加载文档
loader = PyPDFLoader("document.pdf")
documents = loader.load()

# 文档分割
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", "。", "！", "？", "，", " "]
)
chunks = text_splitter.split_documents(documents)

# 创建向量存储
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)

# 保存和加载
vectorstore.save_local("faiss_index")
loaded_vectorstore = FAISS.load_local("faiss_index", embeddings)

# 相似度搜索
results = vectorstore.similarity_search("你的问题", k=3)
for doc in results:
    print(doc.page_content[:100])
```

**5. Chains（链条）**

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# LCEL方式构建链
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

result = chain.invoke("什么是RAG？")

# 顺序链
from langchain.chains import SequentialChain, LLMChain

# 第一个链：生成大纲
outline_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["topic"],
        template="为{topic}生成文章大纲"
    ),
    output_key="outline"
)

# 第二个链：根据大纲写文章
writing_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["outline"],
        template="根据以下大纲写一篇文章：\n{outline}"
    ),
    output_key="article"
)

# 组合
overall_chain = SequentialChain(
    chains=[outline_chain, writing_chain],
    input_variables=["topic"],
    output_variables=["outline", "article"]
)

result = overall_chain.invoke({"topic": "人工智能的未来"})
```

**6. Agents（智能体）**

```python
from langchain.agents import AgentExecutor, create_react_agent
from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun
from langchain import hub

# 定义工具
search_tool = DuckDuckGoSearchRun()
wikipedia_tool = WikipediaQueryRun()

tools = [search_tool, wikipedia_tool]

# 获取ReAct提示模板
prompt = hub.pull("hwchase17/react")

# 创建Agent
agent = create_react_agent(llm, tools, prompt)

# 创建AgentExecutor
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

# 运行
response = agent_executor.invoke({
    "input": "2024年诺贝尔物理学奖得主是谁？他们的主要贡献是什么？"
})
print(response["output"])
```

### 1.2 LangChain开发实操详解

#### 1.2.1 CASE：搭建本地知识智能客服（理解ReAct）

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader
from langchain.chains import RetrievalQA
from langchain.agents import AgentExecutor, create_react_agent, Tool
from langchain import hub

class IntelligentCustomerService:
    """智能客服系统"""

    def __init__(self, knowledge_dir: str):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)
        self.embeddings = OpenAIEmbeddings()

        # 加载知识库
        self.vectorstore = self._load_knowledge_base(knowledge_dir)

        # 创建检索链
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )

        # 创建Agent
        self.agent = self._create_agent()

    def _load_knowledge_base(self, dir_path: str):
        """加载并索引知识库文档"""
        loader = DirectoryLoader(dir_path, glob="**/*.txt")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        chunks = text_splitter.split_documents(documents)

        vectorstore = FAISS.from_documents(chunks, self.embeddings)
        return vectorstore

    def _create_agent(self):
        """创建ReAct Agent"""
        tools = [
            Tool(
                name="知识库查询",
                func=lambda q: self.qa_chain.invoke({"query": q})["result"],
                description="查询公司产品、服务、政策相关信息。当用户询问公司相关问题时使用。"
            ),
            Tool(
                name="订单查询",
                func=self._query_order,
                description="查询用户订单状态。输入订单号，返回订单信息。"
            ),
            Tool(
                name="人工客服转接",
                func=self._transfer_to_human,
                description="当问题无法解决或用户要求时，转接人工客服。"
            )
        ]

        prompt = hub.pull("hwchase17/react-chat")
        agent = create_react_agent(self.llm, tools, prompt)

        return AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=5
        )

    def _query_order(self, order_id: str) -> str:
        """模拟订单查询"""
        # 实际应用中连接订单数据库
        mock_orders = {
            "ORD001": "已发货，预计3天内送达",
            "ORD002": "处理中，预计24小时内发货"
        }
        return mock_orders.get(order_id.strip(), "未找到该订单")

    def _transfer_to_human(self, reason: str) -> str:
        """转接人工客服"""
        return f"正在为您转接人工客服，原因：{reason}。请稍候..."

    def chat(self, user_input: str) -> str:
        """处理用户咨询"""
        response = self.agent.invoke({"input": user_input})
        return response["output"]


# 使用示例
customer_service = IntelligentCustomerService("./knowledge_base")

# 测试对话
questions = [
    "你们的退货政策是什么？",
    "帮我查一下订单ORD001的状态",
    "我想投诉，帮我转人工"
]

for q in questions:
    print(f"用户: {q}")
    print(f"客服: {customer_service.chat(q)}\n")
```

#### 1.2.2 CASE：故障诊断Agent

```python
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field

class DiagnosticInput(BaseModel):
    """诊断工具输入"""
    system: str = Field(description="系统名称：database/network/application")
    symptom: str = Field(description="故障症状描述")

class MetricQueryInput(BaseModel):
    """指标查询输入"""
    metric_name: str = Field(description="指标名称")
    time_range: str = Field(description="时间范围，如：1h, 24h")

class FaultDiagnosisAgent:
    """故障诊断Agent"""

    def __init__(self, llm):
        self.llm = llm
        self.agent = self._create_agent()

    def diagnose_system(self, system: str, symptom: str) -> dict:
        """诊断系统故障"""
        # 模拟诊断逻辑
        diagnostics = {
            "database": {
                "慢查询": {"原因": "索引缺失", "建议": "添加适当索引"},
                "连接超时": {"原因": "连接池耗尽", "建议": "增加连接池大小"}
            },
            "network": {
                "延迟高": {"原因": "带宽不足", "建议": "扩容带宽或优化流量"},
                "丢包": {"原因": "网络拥塞", "建议": "检查网络设备"}
            }
        }
        return diagnostics.get(system, {}).get(symptom, {"原因": "未知", "建议": "需要进一步分析"})

    def query_metrics(self, metric_name: str, time_range: str) -> dict:
        """查询系统指标"""
        # 模拟指标数据
        import random
        return {
            "metric": metric_name,
            "time_range": time_range,
            "current_value": random.uniform(0, 100),
            "avg_value": random.uniform(0, 100),
            "max_value": random.uniform(50, 100)
        }

    def execute_command(self, command: str) -> str:
        """执行诊断命令（安全沙箱内）"""
        allowed_commands = ["ping", "traceroute", "netstat", "top"]
        cmd_name = command.split()[0]
        if cmd_name not in allowed_commands:
            return f"命令 {cmd_name} 不在允许列表中"
        return f"[模拟执行] {command}: 执行成功"

    def _create_agent(self):
        """创建诊断Agent"""
        tools = [
            StructuredTool.from_function(
                func=self.diagnose_system,
                name="diagnose_system",
                description="诊断指定系统的故障，分析症状并给出建议",
                args_schema=DiagnosticInput
            ),
            StructuredTool.from_function(
                func=self.query_metrics,
                name="query_metrics",
                description="查询系统监控指标",
                args_schema=MetricQueryInput
            ),
            StructuredTool.from_function(
                func=self.execute_command,
                name="execute_command",
                description="执行诊断命令，仅支持安全命令"
            )
        ]

        prompt = ChatPromptTemplate.from_messages([
            ("system", """你是一位资深的SRE工程师，擅长故障诊断和系统运维。
请使用提供的工具来诊断问题，给出详细的分析和解决建议。

诊断流程：
1. 首先查询相关指标，了解系统状态
2. 根据症状使用诊断工具分析
3. 如需要，执行诊断命令获取更多信息
4. 综合分析后给出诊断结论和解决方案"""),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])

        agent = create_structured_chat_agent(self.llm, tools, prompt)

        return AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            max_iterations=10
        )

    def diagnose(self, problem: str) -> str:
        """执行故障诊断"""
        return self.agent.invoke({"input": problem})["output"]


# 使用示例
diagnosis_agent = FaultDiagnosisAgent(llm)
result = diagnosis_agent.diagnose("数据库响应很慢，用户反馈查询超时")
print(result)
```

### 1.3 LCEL构建任务链

#### 1.3.1 LCEL（LangChain Expression Language）

**LCEL的核心概念：**

```python
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableParallel,
    RunnableLambda,
    RunnableBranch
)

# 基础链：prompt | model | parser
basic_chain = prompt | llm | StrOutputParser()

# 并行执行
parallel_chain = RunnableParallel(
    summary=summary_chain,
    keywords=keyword_chain,
    sentiment=sentiment_chain
)

# 条件分支
branch_chain = RunnableBranch(
    (lambda x: "技术" in x["query"], tech_chain),
    (lambda x: "销售" in x["query"], sales_chain),
    default_chain  # 默认分支
)

# 自定义函数
def process_data(input_data):
    return input_data.upper()

custom_chain = RunnableLambda(process_data) | llm

# 传递原始输入
passthrough_chain = RunnableParallel(
    original=RunnablePassthrough(),
    processed=processing_chain
)
```

#### 1.3.2 复杂LCEL链示例

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate

# 构建一个文档分析链
class DocumentAnalyzer:
    def __init__(self, llm):
        self.llm = llm

    def create_chain(self):
        # 摘要生成
        summary_prompt = ChatPromptTemplate.from_template(
            "请用100字总结以下文档：\n\n{document}"
        )
        summary_chain = summary_prompt | self.llm | StrOutputParser()

        # 关键词提取
        keywords_prompt = ChatPromptTemplate.from_template(
            "请从以下文档中提取5个关键词，以JSON列表返回：\n\n{document}"
        )
        keywords_chain = keywords_prompt | self.llm | JsonOutputParser()

        # 情感分析
        sentiment_prompt = ChatPromptTemplate.from_template(
            "分析以下文档的情感倾向（正面/负面/中性）：\n\n{document}"
        )
        sentiment_chain = sentiment_prompt | self.llm | StrOutputParser()

        # 问题生成
        questions_prompt = ChatPromptTemplate.from_template(
            "基于以下文档生成3个可能的问题：\n\n{document}"
        )
        questions_chain = questions_prompt | self.llm | StrOutputParser()

        # 并行执行所有分析
        analysis_chain = RunnableParallel(
            summary=summary_chain,
            keywords=keywords_chain,
            sentiment=sentiment_chain,
            questions=questions_chain,
            original_document=RunnablePassthrough()
        )

        # 最终整合
        integration_prompt = ChatPromptTemplate.from_template("""
基于以下分析结果，生成一份完整的文档分析报告：

摘要：{summary}
关键词：{keywords}
情感：{sentiment}
延伸问题：{questions}

请生成结构化的分析报告：
""")

        final_chain = (
            analysis_chain
            | RunnableLambda(lambda x: {
                "summary": x["summary"],
                "keywords": str(x["keywords"]),
                "sentiment": x["sentiment"],
                "questions": x["questions"]
            })
            | integration_prompt
            | self.llm
            | StrOutputParser()
        )

        return final_chain


# 使用
analyzer = DocumentAnalyzer(llm)
chain = analyzer.create_chain()
result = chain.invoke({"document": "你的文档内容..."})
```

---

## 第二章：AI框架设计与选型

### 2.1 自研框架设计思路

#### 2.1.1 核心组件抽象

**模块化框架设计：**

```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

# 基础接口定义
class BaseLLM(ABC):
    """LLM基类"""

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        pass

    @abstractmethod
    def generate_stream(self, prompt: str, **kwargs):
        pass


class BaseEmbedding(ABC):
    """Embedding基类"""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        pass

    @abstractmethod
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        pass


class BaseTool(ABC):
    """工具基类"""

    @property
    @abstractmethod
    def name(self) -> str:
        pass

    @property
    @abstractmethod
    def description(self) -> str:
        pass

    @abstractmethod
    def run(self, input: Any) -> Any:
        pass


class BaseMemory(ABC):
    """记忆基类"""

    @abstractmethod
    def add(self, key: str, value: Any):
        pass

    @abstractmethod
    def get(self, key: str) -> Any:
        pass

    @abstractmethod
    def clear(self):
        pass


class BaseVectorStore(ABC):
    """向量存储基类"""

    @abstractmethod
    def add(self, texts: List[str], embeddings: List[List[float]], metadatas: List[Dict]):
        pass

    @abstractmethod
    def search(self, query_embedding: List[float], k: int) -> List[Dict]:
        pass
```

#### 2.1.2 插件化架构

```python
class PluginManager:
    """插件管理器"""

    def __init__(self):
        self._plugins: Dict[str, Any] = {}
        self._hooks: Dict[str, List[callable]] = {}

    def register_plugin(self, name: str, plugin: Any):
        """注册插件"""
        self._plugins[name] = plugin
        # 自动发现并注册钩子
        for method_name in dir(plugin):
            if method_name.startswith("on_"):
                hook_name = method_name[3:]  # 去掉 "on_" 前缀
                self.register_hook(hook_name, getattr(plugin, method_name))

    def register_hook(self, hook_name: str, callback: callable):
        """注册钩子函数"""
        if hook_name not in self._hooks:
            self._hooks[hook_name] = []
        self._hooks[hook_name].append(callback)

    def trigger_hook(self, hook_name: str, *args, **kwargs):
        """触发钩子"""
        for callback in self._hooks.get(hook_name, []):
            callback(*args, **kwargs)

    def get_plugin(self, name: str) -> Any:
        """获取插件"""
        return self._plugins.get(name)


# 示例插件
class LoggingPlugin:
    """日志插件"""

    def on_request_start(self, request):
        print(f"[LOG] Request started: {request[:50]}...")

    def on_request_end(self, response):
        print(f"[LOG] Request ended: {response[:50]}...")


class MetricsPlugin:
    """指标插件"""

    def __init__(self):
        self.request_count = 0
        self.total_tokens = 0

    def on_request_end(self, response):
        self.request_count += 1
        self.total_tokens += len(response.split())
```

#### 2.1.3 数据流与控制流设计

```python
from dataclasses import dataclass
from typing import Any, Callable, List
import asyncio

@dataclass
class Message:
    """消息数据结构"""
    role: str  # system, user, assistant, tool
    content: Any
    metadata: Dict = None

class Pipeline:
    """数据流管线"""

    def __init__(self):
        self.steps: List[Callable] = []

    def add_step(self, step: Callable):
        """添加处理步骤"""
        self.steps.append(step)
        return self

    def __or__(self, other):
        """支持 | 操作符"""
        self.add_step(other)
        return self

    def run(self, input_data: Any) -> Any:
        """同步执行"""
        result = input_data
        for step in self.steps:
            result = step(result)
        return result

    async def run_async(self, input_data: Any) -> Any:
        """异步执行"""
        result = input_data
        for step in self.steps:
            if asyncio.iscoroutinefunction(step):
                result = await step(result)
            else:
                result = step(result)
        return result


class ParallelPipeline:
    """并行执行管线"""

    def __init__(self, pipelines: Dict[str, Pipeline]):
        self.pipelines = pipelines

    async def run(self, input_data: Any) -> Dict[str, Any]:
        """并行执行所有管线"""
        tasks = {
            name: asyncio.create_task(pipeline.run_async(input_data))
            for name, pipeline in self.pipelines.items()
        }

        results = {}
        for name, task in tasks.items():
            results[name] = await task

        return results
```

### 2.2 优秀开源框架详解

#### 2.2.1 LlamaIndex深度解析

**LlamaIndex核心概念：**

```
LlamaIndex专注于数据连接和索引，是RAG系统的最佳选择

核心组件：
├── Data Connectors：连接各种数据源
├── Data Index：多种索引结构
├── Query Engine：查询引擎
└── Chat Engine：对话引擎
```

```python
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    ServiceContext,
    StorageContext,
    load_index_from_storage
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# 配置
llm = OpenAI(model="gpt-4", temperature=0)
embed_model = OpenAIEmbedding()

# 加载文档
documents = SimpleDirectoryReader("./data").load_data()

# 创建索引
index = VectorStoreIndex.from_documents(
    documents,
    llm=llm,
    embed_model=embed_model
)

# 持久化
index.storage_context.persist("./storage")

# 从存储加载
storage_context = StorageContext.from_defaults(persist_dir="./storage")
loaded_index = load_index_from_storage(storage_context)

# 查询引擎
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact"  # tree_summarize, refine, compact, simple
)

response = query_engine.query("什么是RAG？")
print(response)

# 对话引擎
chat_engine = index.as_chat_engine(
    chat_mode="condense_plus_context",
    verbose=True
)

response = chat_engine.chat("介绍一下你的知识库")
print(response)
```

#### 2.2.2 AutoGen多智能体框架

**AutoGen核心特点：**

```
AutoGen是微软开源的多Agent协作框架

特点：
├── 多Agent对话：多个Agent之间自动对话
├── 人机协作：Human-in-the-Loop
├── 代码执行：内置安全代码执行
└── 可定制：灵活的Agent配置
```

```python
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

# 配置
config_list = [{"model": "gpt-4", "api_key": "your-key"}]

# 创建助手Agent
assistant = AssistantAgent(
    name="Assistant",
    llm_config={"config_list": config_list},
    system_message="你是一个有帮助的AI助手。"
)

# 创建用户代理（可执行代码）
user_proxy = UserProxyAgent(
    name="User",
    human_input_mode="NEVER",  # ALWAYS, TERMINATE, NEVER
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False  # 生产环境建议True
    }
)

# 单轮对话
user_proxy.initiate_chat(
    assistant,
    message="写一个Python函数计算斐波那契数列"
)

# 多Agent群聊
planner = AssistantAgent(
    name="Planner",
    system_message="你是项目规划师，负责分解任务。",
    llm_config={"config_list": config_list}
)

coder = AssistantAgent(
    name="Coder",
    system_message="你是程序员，负责编写代码。",
    llm_config={"config_list": config_list}
)

reviewer = AssistantAgent(
    name="Reviewer",
    system_message="你是代码审查员，负责审查代码质量。",
    llm_config={"config_list": config_list}
)

# 创建群聊
group_chat = GroupChat(
    agents=[user_proxy, planner, coder, reviewer],
    messages=[],
    max_round=10
)

manager = GroupChatManager(
    groupchat=group_chat,
    llm_config={"config_list": config_list}
)

# 启动群聊
user_proxy.initiate_chat(
    manager,
    message="开发一个简单的待办事项Web应用"
)
```

#### 2.2.3 框架选型对比

```
┌─────────────────────────────────────────────────────────────┐
│              LangChain vs LlamaIndex vs AutoGen             │
├─────────────────┬─────────────────┬─────────────────────────┤
│     维度        │   LangChain     │  LlamaIndex │  AutoGen  │
├─────────────────┼─────────────────┼─────────────┼───────────┤
│  核心定位       │ 通用LLM应用框架  │ 数据索引RAG │ 多Agent   │
│  学习曲线       │ 中等            │ 较低        │ 中等      │
│  灵活性         │ 高              │ 中          │ 高        │
│  RAG支持        │ 好              │ 最佳        │ 一般      │
│  Agent能力      │ 好              │ 一般        │ 最佳      │
│  社区生态       │ 最大            │ 大          │ 中        │
│  生产就绪度     │ 高              │ 高          │ 中        │
├─────────────────┴─────────────────┴─────────────┴───────────┤
│                                                             │
│  选型建议：                                                  │
│  • 通用LLM应用 → LangChain                                  │
│  • 知识库/RAG → LlamaIndex                                  │
│  • 多Agent协作 → AutoGen                                    │
│  • 组合使用：LlamaIndex处理RAG + LangChain构建Agent          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 第三章：HuggingFace生态实战

### 3.1 HuggingFace模型库的使用

#### 3.1.1 核心组件介绍

```python
# Transformers - 模型库
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline
)

# Datasets - 数据集库
from datasets import load_dataset, Dataset

# Tokenizers - 分词器
from tokenizers import Tokenizer

# Accelerate - 分布式训练
from accelerate import Accelerator

# PEFT - 高效微调
from peft import LoraConfig, get_peft_model
```

#### 3.1.2 Pipeline API快速使用

```python
from transformers import pipeline

# 文本生成
generator = pipeline("text-generation", model="gpt2")
result = generator("Once upon a time", max_length=50)
print(result)

# 情感分析
classifier = pipeline("sentiment-analysis")
result = classifier("I love this product!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.99}]

# 问答
qa = pipeline("question-answering")
result = qa(
    question="What is the capital of France?",
    context="Paris is the capital and largest city of France."
)
print(result)  # {'answer': 'Paris', 'score': 0.99}

# 文本摘要
summarizer = pipeline("summarization")
result = summarizer(long_text, max_length=130, min_length=30)

# 翻译
translator = pipeline("translation_en_to_zh", model="Helsinki-NLP/opus-mt-en-zh")
result = translator("Hello, how are you?")

# 零样本分类
zero_shot = pipeline("zero-shot-classification")
result = zero_shot(
    "This is a great movie about science",
    candidate_labels=["entertainment", "science", "politics"]
)

# 中文NLP
chinese_ner = pipeline("ner", model="bert-base-chinese")
chinese_sentiment = pipeline("sentiment-analysis", model="uer/roberta-base-finetuned-jd-binary-chinese")
```

#### 3.1.3 模型和数据集的使用

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

# 加载模型和分词器
model_name = "Qwen/Qwen2-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)

# 推理
text = "人工智能是"
inputs = tokenizer(text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# 加载数据集
dataset = load_dataset("squad")  # 从Hub加载
dataset = load_dataset("json", data_files="data.json")  # 从本地加载
dataset = load_dataset("csv", data_files="data.csv")

# 数据集操作
# 划分
train_test = dataset.train_test_split(test_size=0.2)

# 过滤
filtered = dataset.filter(lambda x: len(x["text"]) > 100)

# 映射
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length")

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 保存和加载
dataset.save_to_disk("./my_dataset")
loaded = load_from_disk("./my_dataset")
```

### 3.2 使用HuggingFace做模型微调

#### 3.2.1 Trainer API标准微调

```python
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# 加载数据和模型
dataset = load_dataset("imdb")
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# 数据预处理
def preprocess(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(preprocess, batched=True)

# 数据收集器
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# 评估函数
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions)
    }

# 训练参数
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=100,
    evaluation_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    fp16=True  # 混合精度训练
)

# 创建Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# 训练
trainer.train()

# 评估
results = trainer.evaluate()
print(results)

# 保存
trainer.save_model("./final_model")
```

#### 3.2.2 PEFT高效微调：LoRA

```python
from peft import LoraConfig, get_peft_model, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
import torch

# 加载基础模型
model_name = "Qwen/Qwen2-1.5B"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# LoRA配置
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # LoRA秩
    lora_alpha=32,  # 缩放因子
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # 目标模块
    bias="none"
)

# 应用LoRA
model = get_peft_model(model, lora_config)

# 查看可训练参数
model.print_trainable_parameters()
# 输出类似：trainable params: 4,194,304 || all params: 1,500,000,000 || trainable%: 0.28%

# 准备数据
def format_instruction(example):
    return f"### 指令:\n{example['instruction']}\n\n### 回答:\n{example['output']}"

# 训练
training_args = TrainingArguments(
    output_dir="./lora_output",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    save_steps=100,
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator
)

trainer.train()

# 保存LoRA权重
model.save_pretrained("./lora_weights")

# 推理时加载
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained(model_name)
model = PeftModel.from_pretrained(base_model, "./lora_weights")

# 合并权重（可选，用于部署）
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./merged_model")
```

---

## 第四章：神经网络基础与TensorFlow实战

### 4.1 神经网络基础

#### 4.1.1 神经网络结构

```
┌─────────────────────────────────────────────────────────────┐
│                   神经网络基本结构                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   输入层          隐藏层           输出层                    │
│                                                             │
│    ○─────────────○─────────────○                           │
│    ○─────────────○─────────────○                           │
│    ○─────────────○─────────────○                           │
│    ○─────────────○                                         │
│                                                             │
│   x1              h1              y1                        │
│   x2              h2              y2                        │
│   x3              h3              y3                        │
│   x4                                                        │
│                                                             │
│   前向传播：z = Wx + b, a = σ(z)                            │
│   反向传播：计算梯度，更新权重                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 4.1.2 激活函数

```python
import numpy as np

# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ReLU
def relu(x):
    return np.maximum(0, x)

# Tanh
def tanh(x):
    return np.tanh(x)

# Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

# GELU (Transformer常用)
def gelu(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
```

**激活函数对比：**

| 激活函数 | 公式 | 优点 | 缺点 | 适用场景 |
|---------|------|------|------|----------|
| Sigmoid | 1/(1+e^-x) | 输出(0,1)，概率解释 | 梯度消失 | 二分类输出层 |
| ReLU | max(0,x) | 计算简单，缓解梯度消失 | Dead ReLU | 隐藏层首选 |
| Tanh | (e^x-e^-x)/(e^x+e^-x) | 输出(-1,1)，零中心 | 梯度消失 | LSTM/GRU |
| Softmax | e^xi/Σe^xj | 多分类概率分布 | 计算量大 | 多分类输出层 |
| GELU | x·Φ(x) | 平滑非线性 | 计算复杂 | Transformer |

#### 4.1.3 损失函数

```python
import numpy as np

# 均方误差 (回归)
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 二元交叉熵 (二分类)
def binary_cross_entropy(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# 类别交叉熵 (多分类)
def categorical_cross_entropy(y_true, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
```

#### 4.1.4 使用NumPy搭建神经网络

```python
import numpy as np

class NeuralNetwork:
    """从零实现的简单神经网络"""

    def __init__(self, layers):
        """
        layers: [input_size, hidden1, hidden2, ..., output_size]
        """
        self.weights = []
        self.biases = []

        for i in range(len(layers) - 1):
            # Xavier初始化
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, X):
        """前向传播"""
        self.activations = [X]
        self.z_values = []

        current = X
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            z = np.dot(current, w) + b
            self.z_values.append(z)

            if i == len(self.weights) - 1:
                current = self.softmax(z)  # 输出层用softmax
            else:
                current = self.relu(z)  # 隐藏层用ReLU

            self.activations.append(current)

        return current

    def backward(self, X, y, learning_rate=0.01):
        """反向传播"""
        m = X.shape[0]

        # 输出层误差
        delta = self.activations[-1] - y

        # 反向计算梯度
        for i in range(len(self.weights) - 1, -1, -1):
            # 计算梯度
            dW = np.dot(self.activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m

            # 更新权重
            self.weights[i] -= learning_rate * dW
            self.biases[i] -= learning_rate * db

            # 计算前一层误差
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(self.z_values[i-1])

    def train(self, X, y, epochs=1000, learning_rate=0.01, batch_size=32):
        """训练网络"""
        for epoch in range(epochs):
            # Mini-batch
            indices = np.random.permutation(X.shape[0])
            for start in range(0, X.shape[0], batch_size):
                end = start + batch_size
                batch_indices = indices[start:end]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # 前向 + 反向
                self.forward(X_batch)
                self.backward(X_batch, y_batch, learning_rate)

            # 打印损失
            if epoch % 100 == 0:
                pred = self.forward(X)
                loss = -np.mean(y * np.log(pred + 1e-15))
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

    def predict(self, X):
        """预测"""
        return np.argmax(self.forward(X), axis=1)


# 使用示例
# 创建网络: 输入784维，两个隐藏层(128, 64)，输出10类
nn = NeuralNetwork([784, 128, 64, 10])

# 训练
nn.train(X_train, y_train_onehot, epochs=1000, learning_rate=0.01)

# 预测
predictions = nn.predict(X_test)
```

### 4.2 TensorFlow/Keras实战

#### 4.2.1 使用Keras构建神经网络

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models

# 构建Sequential模型
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# 编译模型
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 查看模型结构
model.summary()

# 训练
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
        keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)
    ]
)

# 评估
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_acc:.4f}")

# 预测
predictions = model.predict(X_test)
```

#### 4.2.2 Functional API构建复杂模型

```python
from tensorflow.keras import layers, Model, Input

# 多输入多输出模型
# 输入1: 文本特征
text_input = Input(shape=(100,), name='text_input')
x1 = layers.Dense(64, activation='relu')(text_input)
x1 = layers.Dense(32, activation='relu')(x1)

# 输入2: 数值特征
numeric_input = Input(shape=(10,), name='numeric_input')
x2 = layers.Dense(32, activation='relu')(numeric_input)
x2 = layers.Dense(16, activation='relu')(x2)

# 合并
merged = layers.concatenate([x1, x2])
x = layers.Dense(64, activation='relu')(merged)

# 多输出
classification_output = layers.Dense(5, activation='softmax', name='classification')(x)
regression_output = layers.Dense(1, name='regression')(x)

# 创建模型
model = Model(
    inputs=[text_input, numeric_input],
    outputs=[classification_output, regression_output]
)

# 编译
model.compile(
    optimizer='adam',
    loss={
        'classification': 'sparse_categorical_crossentropy',
        'regression': 'mse'
    },
    loss_weights={'classification': 1.0, 'regression': 0.5},
    metrics={
        'classification': 'accuracy',
        'regression': 'mae'
    }
)

# 训练
model.fit(
    {'text_input': text_data, 'numeric_input': numeric_data},
    {'classification': labels, 'regression': values},
    epochs=10
)
```

---

## 第五章：PyTorch与视觉检测

### 5.1 PyTorch核心概念

#### 5.1.1 张量与自动求导

```python
import torch
import torch.nn as nn

# 创建张量
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0, 6.0])

# 运算
z = x * y + x ** 2
loss = z.sum()

# 自动求导
loss.backward()
print(x.grad)  # 梯度: dy/dx

# 常用张量操作
a = torch.randn(3, 4)  # 随机张量
b = torch.zeros(3, 4)  # 全零
c = torch.ones(3, 4)   # 全一
d = a.view(4, 3)       # 改变形状
e = a.to('cuda')       # 移动到GPU
```

#### 5.1.2 构建神经网络

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)
        self.fc3 = nn.Linear(hidden_size // 2, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out


# 创建模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleNet(784, 256, 10).to(device)

# 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练循环
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        # 前向传播
        output = model(data)
        loss = criterion(output, target)

        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')

# 评估
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()

    print(f'Accuracy: {100 * correct / total:.2f}%')
```

### 5.2 YOLO视觉检测

#### 5.2.1 YOLO发展历程

```
YOLO演进：
├── YOLOv1 (2015): 开创性的单阶段检测
├── YOLOv2 (2016): 引入Batch Norm和锚框
├── YOLOv3 (2018): 多尺度检测，FPN
├── YOLOv4 (2020): CSP、Mish激活函数
├── YOLOv5 (2020): PyTorch实现，易用性提升
├── YOLOv6 (2022): 美团开源
├── YOLOv7 (2022): 重参数化
├── YOLOv8 (2023): Ultralytics，SOTA
├── YOLOv9 (2024): 可编程梯度信息
└── YOLOv10+ (2024): 持续优化
```

#### 5.2.2 使用Ultralytics YOLO

```python
from ultralytics import YOLO
import cv2

# 加载预训练模型
model = YOLO('yolov8n.pt')  # nano版本
# model = YOLO('yolov8s.pt')  # small
# model = YOLO('yolov8m.pt')  # medium
# model = YOLO('yolov8l.pt')  # large
# model = YOLO('yolov8x.pt')  # extra large

# 图像检测
results = model('image.jpg')

# 显示结果
for result in results:
    boxes = result.boxes  # 检测框
    masks = result.masks  # 分割掩码（如果有）
    probs = result.probs  # 分类概率（如果有）

    # 可视化
    annotated = result.plot()
    cv2.imshow('Result', annotated)
    cv2.waitKey(0)

    # 保存
    result.save('output.jpg')

# 视频检测
results = model('video.mp4', stream=True)
for result in results:
    annotated = result.plot()
    cv2.imshow('Video', annotated)
    if cv2.waitKey(1) == ord('q'):
        break

# 实时摄像头检测
results = model(0, stream=True)  # 0 = 默认摄像头
```

#### 5.2.3 训练自定义YOLO模型

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')

# 训练
results = model.train(
    data='dataset.yaml',  # 数据集配置
    epochs=100,
    imgsz=640,
    batch=16,
    device=0,  # GPU
    workers=4,
    patience=20,  # 早停
    project='runs/detect',
    name='my_model'
)

# dataset.yaml 格式
"""
path: /path/to/dataset
train: images/train
val: images/val
test: images/test

names:
  0: class1
  1: class2
  2: class3
"""

# 验证
metrics = model.val()

# 导出
model.export(format='onnx')  # 支持: onnx, torchscript, tflite等

# 使用训练好的模型
trained_model = YOLO('runs/detect/my_model/weights/best.pt')
results = trained_model('test_image.jpg')
```

---

## 专项求职辅导

### 开发框架相关面试问题

**1. LangChain解决了什么问题？**

```
LangChain解决的核心问题：

1. 组件标准化
   - 统一的模型接口
   - 统一的工具定义
   - 标准化的数据流

2. 快速原型
   - 开箱即用的组件
   - 减少重复开发

3. 复杂应用编排
   - Chain和Agent抽象
   - 支持复杂工作流

4. 生态集成
   - 大量第三方集成
   - 社区贡献的工具
```

**2. LangChain vs LlamaIndex 核心定位有何不同？**

```
LangChain：
├── 定位：通用LLM应用框架
├── 强项：Agent、工具调用、复杂链
└── 适用：需要多种工具和复杂逻辑的应用

LlamaIndex：
├── 定位：数据框架，专注RAG
├── 强项：数据索引、多种Index结构、查询优化
└── 适用：知识库、文档问答类应用

组合使用：
用LlamaIndex构建高质量的RAG组件
用LangChain构建Agent并集成RAG
```

**3. 什么是PEFT（高效微调）？**

```
PEFT (Parameter-Efficient Fine-Tuning)

核心思想：
只训练少量参数，达到接近全量微调的效果

主要方法：
├── LoRA：低秩分解，在权重矩阵旁添加小矩阵
├── QLoRA：量化基础模型 + LoRA
├── Prefix Tuning：在输入前添加可学习的虚拟token
├── Prompt Tuning：学习软提示向量
└── Adapter：在层间插入小型适配器模块

优势：
├── 显存占用大幅降低（可用消费级GPU）
├── 训练速度快
├── 存储成本低（只保存增量参数）
└── 可以为同一基座模型训练多个任务适配器
```

---

## 本模块总结

### 核心能力清单

1. **LangChain**：掌握六大组件，能构建RAG和Agent应用
2. **LCEL**：熟练使用表达式语言构建复杂链
3. **框架选型**：理解不同框架的定位和适用场景
4. **HuggingFace**：会用Pipeline，能做LoRA微调
5. **神经网络**：理解基本原理，会用Keras/PyTorch
6. **YOLO**：能训练和部署目标检测模型

### 实践建议

1. 多做实际项目，理解框架设计思想
2. 关注框架更新，特别是LangChain的LCEL演进
3. 根据场景选择框架，不要过度设计
4. 掌握底层原理，才能更好地使用框架
