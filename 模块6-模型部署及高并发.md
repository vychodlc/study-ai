# 模块6: 模型部署及高并发

## 课程概述

本模块深入讲解大模型的企业级部署技术，从硬件选型、部署框架选择，到高并发服务原理、性能调优，帮助学员掌握将AI模型高效部署到生产环境的完整能力。

---

## 第一章：企业级AI部署：从硬件选型到框架选择

### 1.1 企业级AI硬件的选型、规划与优化

#### 1.1.1 GPU选型策略

**主流GPU对比：**

```
┌─────────────────────────────────────────────────────────────┐
│                   主流GPU对比                               │
├──────────┬───────┬────────┬────────┬─────────┬─────────────┤
│   型号   │ 显存  │ FP16   │ 内存带宽│  功耗   │   定位      │
├──────────┼───────┼────────┼────────┼─────────┼─────────────┤
│ H100 SXM │ 80GB  │ 1979T  │ 3.35TB/s│  700W  │ 顶级训练/推理│
│ H100 PCIe│ 80GB  │ 1513T  │ 2.0TB/s │  350W  │ 企业推理    │
│ A100 80G │ 80GB  │ 312T   │ 2.0TB/s │  300W  │ 通用训练推理│
│ A100 40G │ 40GB  │ 312T   │ 1.6TB/s │  250W  │ 主流选择    │
│ L40S     │ 48GB  │ 362T   │ 864GB/s │  350W  │ 推理优化    │
│ A10      │ 24GB  │ 125T   │ 600GB/s │  150W  │ 推理入门    │
│ RTX 4090 │ 24GB  │ 330T   │ 1.0TB/s │  450W  │ 消费级顶配  │
│ RTX 3090 │ 24GB  │ 142T   │ 936GB/s │  350W  │ 开发测试    │
├──────────┴───────┴────────┴────────┴─────────┴─────────────┤
│                                                             │
│  选型建议：                                                  │
│  ├── 大规模训练：H100 > A100                                │
│  ├── 企业推理：A100/L40S/A10                                │
│  ├── 开发测试：RTX 4090/3090                                │
│  └── 预算有限：A10/RTX 4090                                 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**模型与GPU显存匹配：**

| 模型规模 | FP16推理 | INT8推理 | INT4推理 | 推荐GPU |
|---------|---------|---------|---------|---------|
| 7B | 14GB | 7GB | 4GB | RTX 4090/A10 |
| 13B | 26GB | 13GB | 7GB | A100 40G |
| 30B | 60GB | 30GB | 15GB | A100 80G |
| 70B | 140GB | 70GB | 35GB | 2×A100 80G |

#### 1.1.2 CPU与内存配比

```
┌─────────────────────────────────────────────────────────────┐
│                CPU与内存规划                                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  CPU的作用：                                                 │
│  ├── Tokenization预处理                                     │
│  ├── 请求调度与队列管理                                      │
│  ├── 数据预处理和后处理                                      │
│  └── 系统级任务（日志、监控等）                              │
│                                                             │
│  配比建议：                                                  │
│  ├── 每GPU配16-32核CPU                                      │
│  ├── CPU主频 > 2.5GHz                                       │
│  └── 推理服务对CPU要求相对较低                               │
│                                                             │
│  内存规划：                                                  │
│  ├── 最小：2倍GPU显存                                       │
│  ├── 推荐：4倍GPU显存                                       │
│  ├── 用途：模型加载缓冲、数据预处理                          │
│  └── ECC内存更可靠                                          │
│                                                             │
│  示例配置（4×A100 80G服务器）：                              │
│  ├── CPU：2×AMD EPYC 7763（128核）                         │
│  ├── 内存：1TB DDR4                                         │
│  ├── GPU：4×A100 80GB                                      │
│  └── 存储：2TB NVMe SSD                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 1.1.3 网络基础设施

**多卡/多节点推理的网络需求：**

```
网络技术对比：

1. TCP/IP（传统以太网）
   - 带宽：10-100Gbps
   - 延迟：~10-100μs
   - 成本：低
   - 适用：单节点推理

2. RoCE（RDMA over Converged Ethernet）
   - 带宽：100-400Gbps
   - 延迟：~2-5μs
   - 成本：中等
   - 适用：多节点推理

3. InfiniBand
   - 带宽：200-400Gbps
   - 延迟：~1-2μs
   - 成本：高
   - 适用：大规模训练/推理

NVLink（卡间互联）：
   - 带宽：600GB/s（NVLink 4.0）
   - 适用：同节点多卡通信
   - 张量并行的关键
```

#### 1.1.4 服务器与集群规划

```python
def plan_cluster(model_size_b, target_qps, max_latency_ms):
    """
    规划推理集群

    Args:
        model_size_b: 模型大小（十亿参数）
        target_qps: 目标QPS
        max_latency_ms: 最大延迟（毫秒）
    """
    # 估算单卡性能（粗略）
    # 以A100 80G为例，7B模型约100 tokens/s
    single_gpu_tps = 100 * (7 / model_size_b)

    # 假设平均输出50 tokens
    avg_output_tokens = 50
    single_gpu_qps = single_gpu_tps / avg_output_tokens

    # 需要的GPU数量
    gpus_for_throughput = target_qps / single_gpu_qps

    # 考虑模型需要的GPU（张量并行）
    gpus_per_instance = 1
    if model_size_b > 30:
        gpus_per_instance = 2
    if model_size_b > 65:
        gpus_per_instance = 4
    if model_size_b > 130:
        gpus_per_instance = 8

    # 实例数
    num_instances = max(1, int(gpus_for_throughput / gpus_per_instance))
    total_gpus = num_instances * gpus_per_instance

    print(f"模型: {model_size_b}B")
    print(f"目标QPS: {target_qps}")
    print(f"每实例GPU数: {gpus_per_instance}")
    print(f"推荐实例数: {num_instances}")
    print(f"总GPU数: {total_gpus}")

    return {
        "gpus_per_instance": gpus_per_instance,
        "num_instances": num_instances,
        "total_gpus": total_gpus
    }

# 示例
plan_cluster(model_size_b=70, target_qps=10, max_latency_ms=5000)
```

---

### 1.2 部署框架Ollama、vLLM、SGLang的特点

#### 1.2.1 Ollama：易用性优先

```bash
# Ollama安装与使用
# 安装
curl -fsSL https://ollama.com/install.sh | sh

# 拉取模型
ollama pull llama2
ollama pull qwen:7b

# 运行模型
ollama run qwen:7b

# API调用
curl http://localhost:11434/api/generate -d '{
  "model": "qwen:7b",
  "prompt": "什么是人工智能？"
}'

# 查看模型列表
ollama list

# 自定义模型（Modelfile）
# Modelfile内容：
# FROM qwen:7b
# PARAMETER temperature 0.7
# SYSTEM "你是一个专业的AI助手"

ollama create my-model -f Modelfile
```

**Ollama特点：**

```
优点：
├── 安装简单，一键部署
├── 支持多种开源模型
├── 自动处理模型下载和管理
├── 适合本地开发和测试
└── 资源占用较低

缺点：
├── 性能不如专业推理框架
├── 不支持高并发场景
├── 缺少生产级特性
└── 定制能力有限

适用场景：
├── 本地开发测试
├── 个人使用
└── 小型应用原型
```

#### 1.2.2 vLLM：高吞吐推理引擎

```python
# vLLM安装
# pip install vllm

from vllm import LLM, SamplingParams

# 加载模型
llm = LLM(
    model="Qwen/Qwen2-7B-Instruct",
    tensor_parallel_size=1,  # GPU数量
    gpu_memory_utilization=0.9,  # 显存利用率
    max_model_len=4096,  # 最大序列长度
)

# 采样参数
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512,
)

# 批量推理
prompts = [
    "什么是机器学习？",
    "解释一下深度学习的原理",
    "Python有哪些优点？"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}\n")
```

**启动vLLM服务器：**

```bash
# 启动API服务器
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2-7B-Instruct \
    --port 8000 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 4096

# 调用API（OpenAI兼容）
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2-7B-Instruct",
    "messages": [{"role": "user", "content": "你好"}],
    "temperature": 0.7
  }'
```

**vLLM特点：**

```
核心优势：
├── PagedAttention：高效显存管理
├── Continuous Batching：动态批处理
├── 高吞吐量：比HuggingFace快5-24倍
├── OpenAI兼容API
└── 支持张量并行

适用场景：
├── 生产环境推理服务
├── 高并发在线服务
└── 大规模批量处理
```

#### 1.2.3 SGLang：复杂任务优化

```python
# SGLang安装
# pip install sglang

import sglang as sgl

# 定义程序
@sgl.function
def multi_turn_qa(s, question1, question2):
    s += sgl.system("你是一个有帮助的AI助手")
    s += sgl.user(question1)
    s += sgl.assistant(sgl.gen("answer1", max_tokens=256))
    s += sgl.user(question2)
    s += sgl.assistant(sgl.gen("answer2", max_tokens=256))

# 运行
runtime = sgl.Runtime(model_path="Qwen/Qwen2-7B-Instruct")
sgl.set_default_backend(runtime)

state = multi_turn_qa.run(
    question1="什么是RAG？",
    question2="它有什么优点？"
)

print(state["answer1"])
print(state["answer2"])
```

**SGLang服务器：**

```bash
# 启动服务器
python -m sglang.launch_server \
    --model-path Qwen/Qwen2-7B-Instruct \
    --port 30000

# 特性：
# - RadixAttention：前缀缓存优化
# - 适合多轮对话和复杂Prompt
# - 高效的控制流支持
```

#### 1.2.4 框架选型对比

```
┌─────────────────────────────────────────────────────────────┐
│              部署框架对比                                    │
├──────────┬─────────────┬─────────────┬─────────────────────┤
│   维度   │   Ollama    │    vLLM     │      SGLang        │
├──────────┼─────────────┼─────────────┼─────────────────────┤
│ 易用性   │ ⭐⭐⭐⭐⭐    │ ⭐⭐⭐⭐      │ ⭐⭐⭐              │
│ 性能     │ ⭐⭐⭐        │ ⭐⭐⭐⭐⭐    │ ⭐⭐⭐⭐⭐          │
│ 并发能力 │ ⭐⭐          │ ⭐⭐⭐⭐⭐    │ ⭐⭐⭐⭐⭐          │
│ 多轮对话 │ ⭐⭐⭐        │ ⭐⭐⭐        │ ⭐⭐⭐⭐⭐          │
│ 生产就绪 │ ⭐⭐          │ ⭐⭐⭐⭐⭐    │ ⭐⭐⭐⭐            │
│ 模型支持 │ ⭐⭐⭐⭐      │ ⭐⭐⭐⭐⭐    │ ⭐⭐⭐⭐            │
├──────────┴─────────────┴─────────────┴─────────────────────┤
│                                                             │
│  选型建议：                                                  │
│  ├── 本地开发/个人使用 → Ollama                             │
│  ├── 生产环境高并发 → vLLM                                  │
│  ├── 多轮对话/Agent → SGLang                                │
│  └── 简单API服务 → vLLM                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 第二章：AI服务核心：高并发原理与性能监控调优

### 2.1 高并发AI服务原理

#### 2.1.1 KV Cache瓶颈

**什么是KV Cache？**

```
┌─────────────────────────────────────────────────────────────┐
│                    KV Cache原理                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Transformer自回归生成过程：                                 │
│                                                             │
│  Token 1 → Token 2 → Token 3 → Token 4 → ...               │
│                                                             │
│  每生成一个新Token，需要计算Attention：                      │
│  Attention(Q, K, V) = softmax(QK^T / √d) × V               │
│                                                             │
│  问题：每次都重新计算所有token的K和V太浪费                   │
│                                                             │
│  解决：缓存历史token的K和V值                                 │
│                                                             │
│  KV Cache存储：                                              │
│  ├── 每层：K矩阵 (seq_len, num_heads, head_dim)            │
│  ├── 每层：V矩阵 (seq_len, num_heads, head_dim)            │
│  └── 总大小：2 × num_layers × seq_len × hidden_dim        │
│                                                             │
│  显存占用示例（7B模型，4096序列长度）：                       │
│  2 × 32层 × 4096 × 4096 × 2字节 = 2GB                      │
│  如果同时服务100个请求 = 200GB！                            │
│                                                             │
│  这就是KV Cache成为显存瓶颈的原因                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 2.1.2 PagedAttention核心思想

**PagedAttention原理：**

```
┌─────────────────────────────────────────────────────────────┐
│                  PagedAttention原理                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  传统方式：预分配连续显存                                    │
│  ┌────────────────────────────────────────────┐            │
│  │ Request 1: ████████░░░░░░░░ (预留但未使用)  │            │
│  │ Request 2: ██████░░░░░░░░░░ (预留但未使用)  │            │
│  │ Request 3: ████████████░░░░ (预留但未使用)  │            │
│  └────────────────────────────────────────────┘            │
│  问题：显存碎片化，利用率低                                  │
│                                                             │
│  PagedAttention：按页分配，类似OS虚拟内存                    │
│                                                             │
│  物理Block池：                                               │
│  ┌───┬───┬───┬───┬───┬───┬───┬───┐                        │
│  │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │ 8 │                        │
│  └───┴───┴───┴───┴───┴───┴───┴───┘                        │
│                                                             │
│  逻辑映射：                                                  │
│  Request 1: [Block 1] → [Block 4] → [Block 7]              │
│  Request 2: [Block 2] → [Block 5]                          │
│  Request 3: [Block 3] → [Block 6] → [Block 8]              │
│                                                             │
│  优势：                                                      │
│  ├── 按需分配，消除预留浪费                                  │
│  ├── 支持请求间共享（如共同前缀）                            │
│  ├── 显存利用率从50-60%提升到95%+                           │
│  └── 支持更多并发请求                                       │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 2.1.3 Continuous Batching

**连续批处理原理：**

```
┌─────────────────────────────────────────────────────────────┐
│               Continuous Batching                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  传统Static Batching：                                       │
│  时间 →                                                      │
│  ┌────────────────────────────────────────────────┐        │
│  │ Req1: ████████████████████                    │        │
│  │ Req2: ████████████████████                    │        │
│  │ Req3: ████████████████████                    │        │
│  └────────────────────────────────────────────────┘        │
│  等所有请求完成后才能处理下一批                              │
│                                                             │
│  Continuous Batching：                                       │
│  时间 →                                                      │
│  ┌────────────────────────────────────────────────┐        │
│  │ Req1: ████████████ (完成，移除)                │        │
│  │ Req2: ████████████████████                    │        │
│  │ Req3: ████████ (完成)  → Req4: ██████████     │        │
│  │                        → Req5: ████████████   │        │
│  └────────────────────────────────────────────────┘        │
│  请求完成即移除，新请求立即插入                              │
│                                                             │
│  优势：                                                      │
│  ├── GPU利用率大幅提升                                       │
│  ├── 短请求不需要等待长请求                                  │
│  ├── 降低平均延迟                                           │
│  └── 提高整体吞吐量                                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 2.2 AI服务的性能监控及调优

#### 2.2.1 关键性能指标

```python
class LLMMetrics:
    """LLM服务关键指标"""

    def __init__(self):
        self.metrics = {}

    def calculate_metrics(self, requests_data):
        """计算关键指标"""

        # 1. 吞吐量指标
        metrics = {
            # 每秒请求数
            "qps": self.calc_qps(requests_data),

            # 每秒生成Token数
            "tokens_per_second": self.calc_tps(requests_data),

            # 2. 延迟指标
            # 首Token延迟（Time To First Token）
            "ttft_avg": self.calc_ttft(requests_data),
            "ttft_p50": self.calc_ttft_percentile(requests_data, 50),
            "ttft_p99": self.calc_ttft_percentile(requests_data, 99),

            # 每Token延迟（Time Per Output Token）
            "tpot_avg": self.calc_tpot(requests_data),

            # 端到端延迟
            "e2e_latency_avg": self.calc_e2e_latency(requests_data),
            "e2e_latency_p99": self.calc_e2e_percentile(requests_data, 99),

            # 3. 资源利用率
            "gpu_utilization": self.get_gpu_util(),
            "gpu_memory_used": self.get_gpu_memory(),

            # 4. 队列指标
            "queue_length": self.get_queue_length(),
            "queue_wait_time": self.get_queue_wait(),
        }

        return metrics

    def calc_ttft(self, requests_data):
        """计算首Token延迟"""
        ttfts = [r["first_token_time"] - r["request_time"]
                 for r in requests_data]
        return sum(ttfts) / len(ttfts)

    def calc_tpot(self, requests_data):
        """计算每Token生成时间"""
        tpots = []
        for r in requests_data:
            generation_time = r["end_time"] - r["first_token_time"]
            output_tokens = r["output_tokens"]
            if output_tokens > 1:
                tpots.append(generation_time / (output_tokens - 1))
        return sum(tpots) / len(tpots) if tpots else 0
```

**指标含义解读：**

| 指标 | 含义 | 目标值参考 |
|------|------|-----------|
| QPS | 每秒处理请求数 | 取决于业务需求 |
| TPS | 每秒生成Token数 | 越高越好 |
| TTFT | 首Token延迟 | <500ms |
| TPOT | 每Token延迟 | <50ms |
| P99延迟 | 99%请求延迟 | <业务SLA |
| GPU利用率 | GPU使用率 | >80% |

#### 2.2.2 vLLM性能调优

```python
# vLLM调优参数
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen2-7B-Instruct",

    # 显存相关
    gpu_memory_utilization=0.9,  # 显存利用率，越高并发越大
    max_model_len=4096,  # 最大序列长度，影响显存

    # 批处理相关
    max_num_batched_tokens=8192,  # 每批最大token数
    max_num_seqs=256,  # 最大并发序列数

    # 张量并行（多卡）
    tensor_parallel_size=1,  # GPU数量

    # 量化
    quantization="awq",  # 或 "gptq", None

    # 其他优化
    enforce_eager=False,  # False使用CUDA Graph
    enable_prefix_caching=True,  # 启用前缀缓存
)

# 调优建议
"""
1. 显存利用率
   - 默认0.9，可尝试0.95
   - 过高可能OOM

2. max_num_seqs
   - 增加可提高吞吐
   - 但会增加TTFT

3. 量化
   - AWQ/GPTQ可减少50%显存
   - 精度略有损失

4. 前缀缓存
   - 对相似Prompt有效
   - 多轮对话场景收益大

5. CUDA Graph
   - 减少kernel启动开销
   - 对小batch更有效
"""
```

#### 2.2.3 监控工具栈

```python
# Prometheus + Grafana监控

# 1. 暴露指标
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# 定义指标
REQUEST_COUNT = Counter('llm_requests_total', 'Total requests')
REQUEST_LATENCY = Histogram('llm_request_latency_seconds', 'Request latency',
                            buckets=[0.1, 0.5, 1, 2, 5, 10])
GPU_MEMORY = Gauge('llm_gpu_memory_used_bytes', 'GPU memory used')
QUEUE_SIZE = Gauge('llm_queue_size', 'Request queue size')
TOKENS_GENERATED = Counter('llm_tokens_generated_total', 'Tokens generated')

# 启动metrics服务器
start_http_server(9090)

# 2. 在推理代码中记录指标
def inference(prompt):
    REQUEST_COUNT.inc()

    with REQUEST_LATENCY.time():
        result = model.generate(prompt)

    TOKENS_GENERATED.inc(len(result.tokens))
    return result

# 3. Grafana Dashboard配置
dashboard_config = {
    "panels": [
        {"title": "QPS", "expr": "rate(llm_requests_total[1m])"},
        {"title": "P99 Latency", "expr": "histogram_quantile(0.99, llm_request_latency_seconds_bucket)"},
        {"title": "GPU Memory", "expr": "llm_gpu_memory_used_bytes"},
        {"title": "Queue Size", "expr": "llm_queue_size"},
    ]
}
```

---

## 第三章：SGLang深度优化

### 3.1 SGLang的缓存优化及Radix Tree原理

#### 3.1.1 Radix Tree原理

```
┌─────────────────────────────────────────────────────────────┐
│                    Radix Tree（基数树）                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  传统KV Cache：每个请求独立存储                              │
│                                                             │
│  Req1: [System Prompt][User: 问题1][Assistant: ...]         │
│  Req2: [System Prompt][User: 问题2][Assistant: ...]         │
│  Req3: [System Prompt][User: 问题1][更多上下文]...          │
│                                                             │
│  问题：System Prompt重复存储                                 │
│                                                             │
│  Radix Tree解决方案：前缀共享                                │
│                                                             │
│                    [System Prompt]  ← 共享                  │
│                          │                                  │
│            ┌─────────────┼─────────────┐                    │
│            ↓             ↓             ↓                    │
│      [User:问题1]  [User:问题2]  [User:问题3]               │
│            │             │                                  │
│      ┌─────┴─────┐       │                                  │
│      ↓           ↓       ↓                                  │
│  [Assistant]  [更多]  [Assistant]                           │
│                                                             │
│  优势：                                                      │
│  ├── 共同前缀只存一份                                        │
│  ├── 显存节省显著                                           │
│  ├── 新请求可复用已有缓存                                   │
│  └── 多轮对话场景收益大                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 3.1.2 SGLang的RadixAttention

```python
# SGLang RadixAttention使用示例
import sglang as sgl

# 场景1：共享System Prompt的多请求
@sgl.function
def chat(s, user_message):
    # 这个System Prompt会被缓存和复用
    s += sgl.system("你是一个专业的AI助手，擅长回答技术问题。")
    s += sgl.user(user_message)
    s += sgl.assistant(sgl.gen("response", max_tokens=256))

# 批量处理时，System Prompt只计算一次
messages = [
    "什么是Python？",
    "什么是Java？",
    "什么是Go？",
]

# SGLang自动识别共同前缀并复用缓存
for msg in messages:
    result = chat.run(user_message=msg)
    print(result["response"])

# 场景2：多轮对话（历史自动缓存）
@sgl.function
def multi_turn(s, turns):
    s += sgl.system("你是一个有帮助的助手")

    for i, turn in enumerate(turns):
        s += sgl.user(turn["user"])
        s += sgl.assistant(sgl.gen(f"response_{i}", max_tokens=256))

# 连续对话时，前面的轮次自动缓存
conversation = [
    {"user": "你好"},
    {"user": "今天天气怎么样？"},
    {"user": "推荐一些活动"},
]

result = multi_turn.run(turns=conversation)
```

### 3.2 SGLang对复杂AI任务的优化

#### 3.2.1 复杂控制流优化

```python
import sglang as sgl

# RAG + 多步推理
@sgl.function
def rag_with_reasoning(s, question, documents):
    s += sgl.system("你是一个专业的问答助手")

    # 步骤1：分析问题
    s += sgl.user(f"问题：{question}\n请分析这个问题需要什么信息来回答。")
    s += sgl.assistant(sgl.gen("analysis", max_tokens=100))

    # 步骤2：检索相关文档（假设已检索）
    s += sgl.user(f"""
参考文档：
{documents}

请判断这些文档是否包含回答问题所需的信息。
""")
    s += sgl.assistant(sgl.gen("relevance", max_tokens=100))

    # 步骤3：生成答案
    s += sgl.user("请基于以上分析和文档，回答原始问题。")
    s += sgl.assistant(sgl.gen("answer", max_tokens=300))

    # 步骤4：自我检验
    s += sgl.user("请检查你的答案是否准确完整，如有需要请补充。")
    s += sgl.assistant(sgl.gen("verification", max_tokens=200))


# 条件分支
@sgl.function
def conditional_response(s, user_input):
    s += sgl.system("你是一个智能助手")
    s += sgl.user(user_input)

    # 先判断意图
    s += sgl.assistant(sgl.gen("intent_check", max_tokens=50))

    # 根据意图选择不同处理
    intent = s["intent_check"]
    if "技术问题" in intent:
        s += sgl.user("这是一个技术问题，请详细解答。")
        s += sgl.assistant(sgl.gen("tech_answer", max_tokens=500))
    elif "闲聊" in intent:
        s += sgl.user("轻松回应即可。")
        s += sgl.assistant(sgl.gen("casual_answer", max_tokens=100))
    else:
        s += sgl.assistant(sgl.gen("general_answer", max_tokens=300))
```

#### 3.2.2 SGLang vs vLLM执行差异

```
┌─────────────────────────────────────────────────────────────┐
│              SGLang vs vLLM 执行对比                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  场景：多轮对话 + 复杂Prompt                                 │
│                                                             │
│  vLLM执行方式：                                              │
│  ├── 每轮对话作为独立请求                                    │
│  ├── 需要每次发送完整历史                                    │
│  ├── 重复计算历史部分的KV Cache                             │
│  └── 延迟随对话轮次线性增长                                  │
│                                                             │
│  SGLang执行方式：                                            │
│  ├── Radix Tree自动缓存历史                                 │
│  ├── 只计算新增部分                                         │
│  ├── 前缀匹配复用缓存                                       │
│  └── 延迟保持稳定                                           │
│                                                             │
│  性能对比（10轮对话）：                                       │
│  ├── vLLM: 每轮延迟递增（1x, 1.5x, 2x, ...）               │
│  └── SGLang: 每轮延迟稳定（1x, 1x, 1x, ...）               │
│                                                             │
│  适用场景建议：                                              │
│  ├── 简单单轮请求 → vLLM即可                                │
│  ├── 多轮对话/Agent → SGLang更优                            │
│  └── 共享前缀的批量请求 → SGLang更优                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 专项求职辅导

### 部署相关面试问题

**1. LLM推理的主要瓶颈是什么？**

```
主要瓶颈分析：

1. 显存带宽（Memory Bandwidth）
   - 自回归生成是Memory Bound
   - 每生成一个token需要读取全部模型权重
   - 瓶颈：带宽不够，计算单元等待数据

2. KV Cache
   - 存储历史token的K、V值
   - 长序列显存占用巨大
   - 限制并发请求数

3. 解决方案：
   - PagedAttention：高效显存管理
   - 量化：减少权重大小
   - 批处理：增加计算密度
   - Flash Attention：减少显存访问
```

**2. 什么是KV Cache？**

```
KV Cache定义：
存储Transformer Attention层中历史token的Key和Value向量

作用：
- 自回归生成时避免重复计算
- 用空间换时间

占用计算：
KV Cache大小 = 2 × num_layers × seq_len × hidden_dim × batch_size × bytes

为什么是瓶颈：
- 占用大量显存
- 限制最大序列长度
- 限制并发请求数

优化方法：
- PagedAttention：按需分配
- 量化：INT8存储
- 共享前缀：复用公共部分
```

**3. Continuous Batching的原理是什么？**

```
传统Static Batching：
- 收集一批请求
- 等所有请求完成
- 再处理下一批
- 问题：长请求阻塞短请求

Continuous Batching：
- 请求完成即移除
- 新请求立即加入
- 每次迭代都是不同批次
- GPU始终满载

优势：
- GPU利用率高
- 平均延迟低
- 吞吐量高

实现要点：
- 需要PagedAttention配合
- 动态管理KV Cache
- 复杂的调度逻辑
```

---

## 本模块总结

### 核心能力清单

1. **硬件选型**：能根据需求选择合适的GPU和配置
2. **框架选择**：理解Ollama/vLLM/SGLang的差异和适用场景
3. **高并发原理**：理解KV Cache、PagedAttention、Continuous Batching
4. **性能调优**：掌握vLLM调优参数和监控方法
5. **SGLang使用**：能用SGLang构建复杂的多轮对话系统

### 实践建议

1. 先用Ollama快速验证，再用vLLM/SGLang上生产
2. 建立完善的监控体系
3. 根据场景选择合适的框架
4. 关注显存利用率和延迟的平衡
